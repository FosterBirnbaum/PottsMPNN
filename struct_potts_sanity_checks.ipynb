{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e046acd",
   "metadata": {},
   "source": [
    "# PottsMPNN sanity checks\n",
    "\n",
    "This notebook provides quick smoke tests for the Potts/structure losses and optional ESM + FAPE components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a8cc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MSA loss: 0.2342\n",
      "CA loss: 0.0302\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from training.struct_potts_losses import (\n",
    "    msa_similarity_loss,\n",
    "    msa_similarity_loss_esm,\n",
    "    structure_consistency_loss,\n",
    "    msa_similarity_loss_esmc,\n",
    "    structure_fape_loss,\n",
    "    potts_consistency_loss,\n",
    "    expand_etab_dense,\n",
    ")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "B, L, V, M = 2, 8, 22, 4\n",
    "log_probs = torch.log_softmax(torch.randn(B, L, V), dim=-1)\n",
    "msa_tokens = torch.randint(0, V, (B, M, L))\n",
    "msa_mask = torch.ones(B, M, L)\n",
    "seq_mask = torch.ones(B, L)\n",
    "\n",
    "baseline_loss = msa_similarity_loss(log_probs, msa_tokens, msa_mask, seq_mask, margin=0.1)\n",
    "print(f\"Baseline MSA loss: {baseline_loss.item():.4f}\")\n",
    "\n",
    "positions = torch.randn(B, L, 4, 3)\n",
    "X = positions + 0.1 * torch.randn_like(positions)\n",
    "mask = torch.ones(B, L)\n",
    "ca_loss = structure_consistency_loss(positions, X, mask)\n",
    "print(f\"CA loss: {ca_loss.item():.4f}\")\n",
    "\n",
    "# Standalone Potts consistency sanity check.\n",
    "K, H = 3, 21 * 21\n",
    "etab_geom = torch.randn(B, L, K, H)\n",
    "e_idx = torch.randint(0, L, (B, L, K))\n",
    "etab_seq_dense_match = expand_etab_dense(etab_geom, e_idx)\n",
    "loss_potts_match = potts_consistency_loss(etab_geom, e_idx, etab_seq_dense_match, mask)\n",
    "print(f\"Potts loss (matched targets): {loss_potts_match.item():.6f}\")\n",
    "assert torch.isclose(loss_potts_match, torch.tensor(0.0), atol=1e-6)\n",
    "\n",
    "etab_seq_dense_noisy = etab_seq_dense_match + 0.1 * torch.randn_like(etab_seq_dense_match)\n",
    "loss_potts_noisy = potts_consistency_loss(etab_geom, e_idx, etab_seq_dense_noisy, mask)\n",
    "print(f\"Potts loss (noisy targets): {loss_potts_noisy.item():.6f}\")\n",
    "assert loss_potts_noisy > loss_potts_match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e493d353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  3\n",
      "seq:  ACD\n",
      "Boltz2 feature alignment check passed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from training.boltz2_features import build_boltz2_item_feats, collate_boltz2_feats\n",
    "from training.training_struct_potts import validate_boltz2_alignment\n",
    "\n",
    "dummy_item = {\n",
    "    'seq': 'ACD',\n",
    "    'chain_order': ['A'],\n",
    "    'chain_lengths': [3],\n",
    "    'seq_chain_A': 'ACD',\n",
    "    'atom14_xyz': np.zeros((3, 14, 3), dtype=np.float32),\n",
    "    'atom14_mask': np.ones((3, 14), dtype=np.float32),\n",
    "}\n",
    "dummy_feats = build_boltz2_item_feats(dummy_item)\n",
    "batched_feats = collate_boltz2_feats([dummy_feats])\n",
    "validate_boltz2_alignment(batched_feats, torch.ones(1, 3))\n",
    "assert batched_feats['msa'].shape[-1] == 3\n",
    "print('Boltz2 feature alignment check passed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7925309e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 57456.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESM-C MSA loss: 0.1013, grad norm: 0.0031\n"
     ]
    }
   ],
   "source": [
    "# Optional: ESM-C-based MSA similarity loss\n",
    "try:\n",
    "    import esm\n",
    "    from training.training_struct_potts import build_esm_token_map\n",
    "    from training.struct_potts_losses import msa_similarity_loss_esmc\n",
    "\n",
    "    from esm.models.esmc import ESMC\n",
    "    model = ESMC.from_pretrained(\"esmc_300m\")\n",
    "    model.eval().cpu()\n",
    "    tokenizer = model.tokenizer\n",
    "    token_map = build_esm_token_map(tokenizer, 'ACDEFGHIKLMNPQRSTVWYX-')\n",
    "\n",
    "    log_probs_esmc = log_probs.detach().clone().requires_grad_(True)\n",
    "    esm_loss = msa_similarity_loss_esmc(\n",
    "        log_probs_esmc,\n",
    "        msa_tokens,\n",
    "        msa_mask,\n",
    "        seq_mask,\n",
    "        model,\n",
    "        token_map,\n",
    "        margin=0.1,\n",
    "    )\n",
    "    esm_loss.backward()\n",
    "    grad_norm = log_probs_esmc.grad.norm().item()\n",
    "    print(f\"ESM-C MSA loss: {esm_loss.item():.4f}, grad norm: {grad_norm:.4f}\")\n",
    "except Exception as exc:\n",
    "    print(f\"ESM-C not available: {exc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0203317e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAPE loss: 0.4707\n"
     ]
    }
   ],
   "source": [
    "# Optional: FAPE loss (requires OpenFold)\n",
    "try:\n",
    "    frames = torch.randn(1, B, L, 4, 4)\n",
    "    backbone_4x4 = torch.randn(B, L, 4, 4)\n",
    "    fape_loss = structure_fape_loss(frames, backbone_4x4, mask)\n",
    "    print(f\"FAPE loss: {fape_loss.item():.4f}\")\n",
    "except Exception as exc:\n",
    "    print(f\"OpenFold not available: {exc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4070c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a single batch, run the model, and evaluate all major losses\n",
    "# This path now computes etab_seq_dense the same way as training (Boltz2 trunk + SequencePottsHead).\n",
    "# try:\n",
    "from training.utils import (\n",
    "    worker_init_fn,\n",
    "    loader_pdb,\n",
    "    build_training_clusters,\n",
    "    PDB_dataset,\n",
    "    StructureDataset,\n",
    "    StructureLoader,\n",
    "    get_pdbs,\n",
    ")\n",
    "from training.model_utils_struct import ProteinMPNN, featurize\n",
    "from training.boltz2_adapter import Boltz2TrunkAdapter, SequencePottsHead\n",
    "from training.training_struct_potts import (\n",
    "    load_boltz2_checkpoint,\n",
    "    get_boltz2_feats,\n",
    "    validate_boltz2_alignment,\n",
    ")\n",
    "import os\n",
    "import torch\n",
    "\n",
    "data_path = '/mnt/shared/fosterb/ProteinMPNN/data/pdb_2021aug02'  # update for your local data\n",
    "boltz2_checkpoint = '/mnt/shared/fosterb/boltz2/boltz2.ckpt'  # update for your local checkpoint\n",
    "boltz2_recycles = 1\n",
    "params = {\n",
    "    'LIST': f'{data_path}/list.csv',\n",
    "    'VAL': f'{data_path}/valid_clusters.txt',\n",
    "    'TEST': f'{data_path}/test_clusters.txt',\n",
    "    'DIR': f'{data_path}',\n",
    "    'DATCUT': '2030-Jan-01',\n",
    "    'RESCUT': 3.5,\n",
    "    'HOMO': 0.70,\n",
    "}\n",
    "\n",
    "if not os.path.exists(boltz2_checkpoint):\n",
    "    raise FileNotFoundError(\n",
    "        f'Boltz2 checkpoint not found at {boltz2_checkpoint}. '\n",
    "        'Set boltz2_checkpoint to your local checkpoint to run this integration test.'\n",
    "    )\n",
    "\n",
    "train, _, _ = build_training_clusters(params, debug=True)\n",
    "train_set = PDB_dataset(list(train.keys()), loader_pdb, train, params)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=1, shuffle=True, worker_init_fn=worker_init_fn\n",
    ")\n",
    "\n",
    "pdb_dict_train = get_pdbs(train_loader, repeat=1, max_length=1000, num_units=1)\n",
    "dataset = StructureDataset(pdb_dict_train, truncate=1, max_length=1000)\n",
    "loader = StructureLoader(dataset, batch_size=1)\n",
    "batch = next(iter(loader))\n",
    "\n",
    "# Use the same Boltz2 features consumed in training.\n",
    "boltz2_feats = get_boltz2_feats(batch)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "boltz2_feats = {\n",
    "    k: (v.to(device) if torch.is_tensor(v) else v)\n",
    "    for k, v in boltz2_feats.items()\n",
    "}\n",
    "\n",
    "model = ProteinMPNN(\n",
    "    node_features=128,\n",
    "    edge_features=128,\n",
    "    hidden_dim=128,\n",
    "    num_encoder_layers=3,\n",
    "    num_decoder_layers=3,\n",
    "    k_neighbors=48,\n",
    "    dropout=0.1,\n",
    "    augment_eps=0.2,\n",
    "    use_potts=True,\n",
    "    struct_predict=True,\n",
    "    struct_use_decoder_one_hot=True,\n",
    ").to(device)\n",
    "model.eval()\n",
    "\n",
    "boltz2_model = load_boltz2_checkpoint(boltz2_checkpoint, device)\n",
    "boltz2_model = boltz2_model.to(device)\n",
    "boltz2_model.eval()\n",
    "boltz2_trunk = Boltz2TrunkAdapter.from_boltz2_model(boltz2_model).to(device)\n",
    "seq_potts_head = SequencePottsHead(\n",
    "    pair_dim=boltz2_model.hparams.token_z,\n",
    "    potts_dim=400,\n",
    ").to(device)\n",
    "seq_potts_head.eval()\n",
    "\n",
    "# Build OpenFold backbones so FAPE can be tested on the same batch.\n",
    "(\n",
    "    X,\n",
    "    S,\n",
    "    _,\n",
    "    mask,\n",
    "    lengths,\n",
    "    chain_M,\n",
    "    residue_idx,\n",
    "    mask_self,\n",
    "    chain_encoding_all,\n",
    "    _,\n",
    "    backbone_4x4,\n",
    "    _,\n",
    ") = featurize(\n",
    "    batch,\n",
    "    device,\n",
    "    augment_type='atomic',\n",
    "    augment_eps=0.2,\n",
    "    replicate=1,\n",
    "    epoch=0,\n",
    "    openfold_backbone=True,\n",
    ")\n",
    "\n",
    "X = X.to(device)\n",
    "S = S.to(device)\n",
    "mask = mask.to(device)\n",
    "chain_M = chain_M.to(device)\n",
    "residue_idx = residue_idx.to(device)\n",
    "mask_self = mask_self.to(device)\n",
    "chain_encoding_all = chain_encoding_all.to(device)\n",
    "backbone_4x4 = backbone_4x4.to(device)\n",
    "\n",
    "validate_boltz2_alignment(boltz2_feats, mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    trunk_out = boltz2_trunk(boltz2_feats, boltz2_recycles)\n",
    "    etab_seq_dense = seq_potts_head(trunk_out.z_trunk)\n",
    "\n",
    "    log_probs, etab_geom, e_idx, frames, positions, logits = model(\n",
    "        X,\n",
    "        S,\n",
    "        mask,\n",
    "        chain_M,\n",
    "        residue_idx,\n",
    "        chain_encoding_all,\n",
    "        return_logits=True,\n",
    "    )\n",
    "\n",
    "print(f'positions shape: {None if positions is None else tuple(positions.shape)}')\n",
    "print('The first dimension in positions is trajectory/recycle step; the loss uses the final step.')\n",
    "\n",
    "loss_msa = msa_similarity_loss(\n",
    "    log_probs,\n",
    "    boltz2_feats['msa'],\n",
    "    boltz2_feats['msa_mask'],\n",
    "    mask,\n",
    ")\n",
    "\n",
    "# Main-path Potts check: use the same Boltz2-derived dense Potts target as training.\n",
    "loss_potts = potts_consistency_loss(etab_geom, e_idx, etab_seq_dense, mask)\n",
    "loss_struct_ca = structure_consistency_loss(positions, X, mask)\n",
    "loss_struct_fape = structure_fape_loss(frames, backbone_4x4, mask)\n",
    "print(\n",
    "    f'Losses -> MSA: {loss_msa.item():.4f}, '\n",
    "    f'Potts: {loss_potts.item():.4f}, '\n",
    "    f'Struct (CA): {loss_struct_ca.item():.4f}, '\n",
    "    f'Struct (FAPE): {loss_struct_fape.item():.4f}'\n",
    ")\n",
    "assert torch.isfinite(loss_potts)\n",
    "# except Exception as exc:\n",
    "#     print(f'Full model smoke test skipped: {exc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a1e629-849c-442a-928f-954f17a8ed79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PottsMPNN",
   "language": "python",
   "name": "pottsmpnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}