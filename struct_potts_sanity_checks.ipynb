{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e046acd",
   "metadata": {},
   "source": [
    "# PottsMPNN sanity checks\n",
    "\n",
    "This notebook provides quick smoke tests for the Potts/structure losses and optional ESM + FAPE components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a8cc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MSA loss: 0.2342\n",
      "CA loss: 0.0302\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from training.struct_potts_losses import (\n",
    "    msa_similarity_loss,\n",
    "    msa_similarity_loss_esm,\n",
    "    structure_consistency_loss,\n",
    "    msa_similarity_loss_esmc,\n",
    "    structure_fape_loss,\n",
    ")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "B, L, V, M = 2, 8, 22, 4\n",
    "log_probs = torch.log_softmax(torch.randn(B, L, V), dim=-1)\n",
    "msa_tokens = torch.randint(0, V, (B, M, L))\n",
    "msa_mask = torch.ones(B, M, L)\n",
    "seq_mask = torch.ones(B, L)\n",
    "\n",
    "baseline_loss = msa_similarity_loss(log_probs, msa_tokens, msa_mask, seq_mask, margin=0.1)\n",
    "print(f\"Baseline MSA loss: {baseline_loss.item():.4f}\")\n",
    "\n",
    "positions = torch.randn(B, L, 4, 3)\n",
    "X = positions + 0.1 * torch.randn_like(positions)\n",
    "mask = torch.ones(B, L)\n",
    "ca_loss = structure_consistency_loss(positions, X, mask)\n",
    "print(f\"CA loss: {ca_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e493d353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boltz2 feature alignment check passed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from training.boltz2_features import build_boltz2_item_feats, collate_boltz2_feats\n",
    "from training.training_struct_potts import validate_boltz2_alignment\n",
    "\n",
    "dummy_item = {\n",
    "    'seq': 'ACD',\n",
    "    'chain_order': ['A'],\n",
    "    'chain_lengths': [3],\n",
    "    'seq_chain_A': 'ACD',\n",
    "    'atom14_xyz': np.zeros((3, 14, 3), dtype=np.float32),\n",
    "    'atom14_mask': np.ones((3, 14), dtype=np.float32),\n",
    "}\n",
    "dummy_feats = build_boltz2_item_feats(dummy_item)\n",
    "batched_feats = collate_boltz2_feats([dummy_feats])\n",
    "validate_boltz2_alignment(batched_feats, torch.ones(1, 3))\n",
    "assert batched_feats['msa'].shape[-1] == 3\n",
    "print('Boltz2 feature alignment check passed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7925309e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESM-C MSA loss: 0.0839, grad norm: 0.0070\n"
     ]
    }
   ],
   "source": [
    "# Optional: ESM-C-based MSA similarity loss\n",
    "try:\n",
    "    import esm\n",
    "    from training.training_struct_potts import build_esm_token_map\n",
    "    from training.struct_potts_losses import msa_similarity_loss_esmc\n",
    "\n",
    "    from esm.models.esmc import ESMC\n",
    "    model = ESMC.from_pretrained(\"esmc_300m\")\n",
    "    model.eval().cpu()\n",
    "    tokenizer = model.tokenizer\n",
    "    token_map = build_esm_token_map(tokenizer, 'ACDEFGHIKLMNPQRSTVWYX-')\n",
    "\n",
    "    log_probs_esmc = log_probs.detach().clone().requires_grad_(True)\n",
    "    esm_loss = msa_similarity_loss_esmc(\n",
    "        log_probs_esmc,\n",
    "        msa_tokens,\n",
    "        msa_mask,\n",
    "        seq_mask,\n",
    "        model,\n",
    "        token_map,\n",
    "        margin=0.1,\n",
    "    )\n",
    "    esm_loss.backward()\n",
    "    grad_norm = log_probs_esmc.grad.norm().item()\n",
    "    print(f\"ESM-C MSA loss: {esm_loss.item():.4f}, grad norm: {grad_norm:.4f}\")\n",
    "except Exception as exc:\n",
    "    print(f\"ESM-C not available: {exc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0203317e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenFold not available: No module named 'openfold'\n"
     ]
    }
   ],
   "source": [
    "# Optional: FAPE loss (requires OpenFold)\n",
    "try:\n",
    "    frames = torch.randn(1, B, L, 4, 4)\n",
    "    backbone_4x4 = torch.randn(B, L, 4, 4)\n",
    "    fape_loss = structure_fape_loss(frames, backbone_4x4, mask)\n",
    "    print(f\"FAPE loss: {fape_loss.item():.4f}\")\n",
    "except Exception as exc:\n",
    "    print(f\"OpenFold not available: {exc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4070c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a single batch, run the model, and evaluate losses\n",
    "try:\n",
    "    from training.utils import (\n",
    "        worker_init_fn,\n",
    "        loader_pdb,\n",
    "        build_training_clusters,\n",
    "        PDB_dataset,\n",
    "        StructureDataset,\n",
    "        StructureLoader,\n",
    "    )\n",
    "    from training.model_utils_struct import ProteinMPNN, featurize\n",
    "    import torch\n",
    "\n",
    "    data_path = 'my_path/pdb_2021aug02'  # update for your local data\n",
    "    params = {\n",
    "        'LIST': f'{data_path}/list.csv',\n",
    "        'VAL': f'{data_path}/valid_clusters.txt',\n",
    "        'TEST': f'{data_path}/test_clusters.txt',\n",
    "        'DIR': f'{data_path}',\n",
    "        'DATCUT': '2030-Jan-01',\n",
    "        'RESCUT': 3.5,\n",
    "        'HOMO': 0.70,\n",
    "    }\n",
    "\n",
    "    train, _, _ = build_training_clusters(params, debug=True)\n",
    "    train_set = PDB_dataset(list(train.keys()), loader_pdb, train, params)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=1, shuffle=True, worker_init_fn=worker_init_fn\n",
    "    )\n",
    "\n",
    "    pdb_dict = next(iter(train_loader))\n",
    "    dataset = StructureDataset(pdb_dict, truncate=None, max_length=1000)\n",
    "    loader = StructureLoader(dataset, batch_size=1)\n",
    "    batch = next(iter(loader))\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model = ProteinMPNN(\n",
    "        node_features=128,\n",
    "        edge_features=128,\n",
    "        hidden_dim=128,\n",
    "        num_encoder_layers=3,\n",
    "        num_decoder_layers=3,\n",
    "        k_neighbors=48,\n",
    "        dropout=0.1,\n",
    "        augment_eps=0.2,\n",
    "        use_potts=True,\n",
    "        struct_predict=True,\n",
    "        struct_use_decoder_one_hot=True,\n",
    "    ).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    (\n",
    "        X,\n",
    "        S,\n",
    "        _,\n",
    "        mask,\n",
    "        lengths,\n",
    "        chain_M,\n",
    "        residue_idx,\n",
    "        mask_self,\n",
    "        chain_encoding_all,\n",
    "        _,\n",
    "        backbone_4x4,\n",
    "        _,\n",
    "    ) = featurize(\n",
    "        batch,\n",
    "        device,\n",
    "        augment_type='atomic',\n",
    "        augment_eps=0.2,\n",
    "        replicate=1,\n",
    "        epoch=0,\n",
    "        openfold_backbone=False,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        log_probs, etab_geom, e_idx, frames, positions, logits = model(\n",
    "            X,\n",
    "            S,\n",
    "            mask,\n",
    "            chain_M,\n",
    "            residue_idx,\n",
    "            chain_encoding_all,\n",
    "            return_logits=True,\n",
    "        )\n",
    "\n",
    "    loss_msa = msa_similarity_loss(log_probs, torch.randint(0, 22, (1, 2, log_probs.shape[1])), torch.ones(1, 2, log_probs.shape[1]), mask)\n",
    "    loss_struct = structure_consistency_loss(positions, X, mask)\n",
    "    print(f'Losses -> MSA: {loss_msa.item():.4f}, Struct: {loss_struct.item():.4f}')\n",
    "except Exception as exc:\n",
    "    print(f'Full model smoke test skipped: {exc}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PottsMPNN",
   "language": "python",
   "name": "pottsmpnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
